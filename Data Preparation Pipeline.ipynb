{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Data Preparation Pipeline\n",
    "The aim of this part of our example ML project workflow, is to build a data preparation pipeline that transforms an initial Pandas DataFrame into a Numpy array that is suitable for training a Scikit-Learn machine learning model with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into Predictors and Labels\n",
    "We start by loading our training data and seperating the input variable data `X` from the labelled data `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16512 entries, 0 to 16511\n",
      "Data columns (total 9 columns):\n",
      "longitude             16512 non-null float64\n",
      "latitude              16512 non-null float64\n",
      "housing_median_age    16512 non-null float64\n",
      "total_rooms           16512 non-null float64\n",
      "total_bedrooms        16351 non-null float64\n",
      "population            16512 non-null float64\n",
      "households            16512 non-null float64\n",
      "median_income         16512 non-null float64\n",
      "ocean_proximity       16512 non-null object\n",
      "dtypes: float64(8), object(1)\n",
      "memory usage: 1.1+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.29</td>\n",
       "      <td>37.81</td>\n",
       "      <td>49.0</td>\n",
       "      <td>844.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>560.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>1.7500</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-123.52</td>\n",
       "      <td>41.01</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1564.0</td>\n",
       "      <td>345.0</td>\n",
       "      <td>517.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>2.1542</td>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.89</td>\n",
       "      <td>40.76</td>\n",
       "      <td>14.0</td>\n",
       "      <td>712.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>2.3958</td>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-118.36</td>\n",
       "      <td>34.26</td>\n",
       "      <td>34.0</td>\n",
       "      <td>3677.0</td>\n",
       "      <td>573.0</td>\n",
       "      <td>1598.0</td>\n",
       "      <td>568.0</td>\n",
       "      <td>6.8380</td>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-121.48</td>\n",
       "      <td>38.57</td>\n",
       "      <td>38.0</td>\n",
       "      <td>2809.0</td>\n",
       "      <td>805.0</td>\n",
       "      <td>1243.0</td>\n",
       "      <td>785.0</td>\n",
       "      <td>1.8512</td>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.29     37.81                49.0        844.0           204.0   \n",
       "1    -123.52     41.01                17.0       1564.0           345.0   \n",
       "2    -122.89     40.76                14.0        712.0           131.0   \n",
       "3    -118.36     34.26                34.0       3677.0           573.0   \n",
       "4    -121.48     38.57                38.0       2809.0           805.0   \n",
       "\n",
       "   population  households  median_income ocean_proximity  \n",
       "0       560.0       152.0         1.7500        NEAR BAY  \n",
       "1       517.0       222.0         2.1542          INLAND  \n",
       "2       270.0        90.0         2.3958          INLAND  \n",
       "3      1598.0       568.0         6.8380       <1H OCEAN  \n",
       "4      1243.0       785.0         1.8512          INLAND  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv('data/data_train.csv')\n",
    "X = train_data.drop(['median_house_value'], axis=1)\n",
    "y = train_data['median_house_value']\n",
    "\n",
    "X.info()\n",
    "X.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Mapping and Variable Selection\n",
    "We define a custom transformer for adapting an arbitrarty DataFrame, so that we can automate the process of:\n",
    "- returning Numpy arrays as required by Scikit-Learn algorithms; and so,\n",
    "- we can select subsets of columns (i.e. input variables).\n",
    "\n",
    "We present the class definition below, but in order to be able to serialise the final pipeline(s) the actual code is imported from a simple Python module called `custom_transformers.py` that exists in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "class DataFrameAdapter(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"DataFrameAdapter\n",
    "    \n",
    "    Class for mapping column-subsets of Pandas DataFrames\n",
    "    to raw Numpy arrays.\n",
    "    \"\"\"\n",
    "    def __init__(self, col_names):\n",
    "        self.col_names = list(col_names)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X[self.col_names].values\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        return self.col_names\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test this transformer on our training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['NEAR BAY'],\n",
       "       ['INLAND'],\n",
       "       ['INLAND'],\n",
       "       ..., \n",
       "       ['<1H OCEAN'],\n",
       "       ['INLAND'],\n",
       "       ['NEAR OCEAN']], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from custom_transformers import DataFrameAdapter\n",
    "\n",
    "# test our custom Transformer class\n",
    "cat_data_adapter = DataFrameAdapter(['ocean_proximity'])\n",
    "cat_data = cat_data_adapter.fit_transform(X)\n",
    "\n",
    "cat_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Categorical Variables\n",
    "Unlike R algorithms that (usually) embed the necessary scaffolding code to automatically deal with categorical data, in Scikit-Learn we will have to write a another custom Transform to perform this task. This class will expect a list of categorical (string) variables (i.e. columns) as input, that will then be turned into factors (integer encoded data), before using a one-hot-encoding algorithm to encode the factors to the 0/1 indicator variables required by most ML algorithms (as well as most 'traditional'statistical methods like OLS regression).\n",
    "\n",
    "Once again we present only the class definition below, so that we are able to serialise the final pipeline(s). The actual code is loaded from a simple Python module called `custom_transformers.py` that exists in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class CategoricalFeatureEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"CategoricalFeatureEncoder\n",
    "    \n",
    "    Class for automating the process of applying \n",
    "    one-hot-encoding to all categorical variables in a Numpy\n",
    "    array of only categorical variables.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        return None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape((-1, 1))\n",
    "        num_vars = X.shape[1]\n",
    "        encoded_vars = [self.__transform_single_cat_var__(var) for var in X.T]\n",
    "        feature_names, features = list(zip(*encoded_vars))\n",
    "        self.feature_names = list(np.concatenate(feature_names, axis=0))\n",
    "        return np.concatenate(features, axis=1)\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return self.feature_names\n",
    "    \n",
    "    def __transform_single_cat_var__(self, cat_feature_col):\n",
    "        feature_names_, int_factors = np.unique(cat_feature_col, return_inverse=True)\n",
    "        one_hot_encoder = OneHotEncoder()\n",
    "        encoded_factors = one_hot_encoder.fit_transform(int_factors.reshape((-1, 1)))\n",
    "        return (feature_names_, encoded_factors.toarray())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test this transformer on our training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column names: ['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from custom_transformers import CategoricalFeatureEncoder\n",
    "\n",
    "# test our custom Transformer class\n",
    "cat_encoder = CategoricalFeatureEncoder()\n",
    "encoded_cat_data = cat_encoder.fit_transform(cat_data)\n",
    "\n",
    "print('column names: {}'.format(cat_encoder.get_feature_names()))\n",
    "encoded_cat_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute Missing Data\n",
    "We now test a missing data imputer configured to use the median value to fill-in missing values (such as those we found in the `total_bedrooms` variable), before we add it to our final data preparation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16512 entries, 0 to 16511\n",
      "Data columns (total 8 columns):\n",
      "longitude             16512 non-null float64\n",
      "latitude              16512 non-null float64\n",
      "housing_median_age    16512 non-null float64\n",
      "total_rooms           16512 non-null float64\n",
      "total_bedrooms        16512 non-null float64\n",
      "population            16512 non-null float64\n",
      "households            16512 non-null float64\n",
      "median_income         16512 non-null float64\n",
      "dtypes: float64(8)\n",
      "memory usage: 1.0 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "longitude             False\n",
       "latitude              False\n",
       "housing_median_age    False\n",
       "total_rooms           False\n",
       "total_bedrooms        False\n",
       "population            False\n",
       "households            False\n",
       "median_income         False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "num_data = X.drop(['ocean_proximity'], axis=1)\n",
    "num_imputer = Imputer(strategy='median')\n",
    "imputed_num_data = num_imputer.fit_transform(num_data)\n",
    "\n",
    "# convert back to DataFrame so we can compare with original using info\n",
    "imputed_num_df = pd.DataFrame(imputed_num_data, columns=num_data.columns)\n",
    "imputed_num_df.info()\n",
    "imputed_num_df.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Numeric Data\n",
    "Machine learning algorithms, whether they are using matrix inversion, gradient descent (or some other optimisation routine to minimise a utiliity function), benefit when all variables have the same units. For example, matrix inversion becomes more stable and gradient descent methods don't suffer from the creation of artificial 'valleys'.\n",
    "\n",
    "We test a simple scaling transformer that we will be able to use in our data preparation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16512.000000</td>\n",
       "      <td>16512.000000</td>\n",
       "      <td>16512.000000</td>\n",
       "      <td>16512.000000</td>\n",
       "      <td>16512.000000</td>\n",
       "      <td>16512.000000</td>\n",
       "      <td>16512.000000</td>\n",
       "      <td>16512.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.474923</td>\n",
       "      <td>0.329694</td>\n",
       "      <td>0.541771</td>\n",
       "      <td>0.067011</td>\n",
       "      <td>0.086265</td>\n",
       "      <td>0.087201</td>\n",
       "      <td>0.093008</td>\n",
       "      <td>0.231966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.199489</td>\n",
       "      <td>0.226864</td>\n",
       "      <td>0.247019</td>\n",
       "      <td>0.055069</td>\n",
       "      <td>0.066894</td>\n",
       "      <td>0.067101</td>\n",
       "      <td>0.070631</td>\n",
       "      <td>0.130148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.252988</td>\n",
       "      <td>0.148778</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.036802</td>\n",
       "      <td>0.047519</td>\n",
       "      <td>0.047975</td>\n",
       "      <td>0.051904</td>\n",
       "      <td>0.142095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.581673</td>\n",
       "      <td>0.182784</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>0.054046</td>\n",
       "      <td>0.069749</td>\n",
       "      <td>0.071227</td>\n",
       "      <td>0.076176</td>\n",
       "      <td>0.209301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.631474</td>\n",
       "      <td>0.550478</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.080269</td>\n",
       "      <td>0.103898</td>\n",
       "      <td>0.105767</td>\n",
       "      <td>0.112957</td>\n",
       "      <td>0.292320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          longitude      latitude  housing_median_age   total_rooms  \\\n",
       "count  16512.000000  16512.000000        16512.000000  16512.000000   \n",
       "mean       0.474923      0.329694            0.541771      0.067011   \n",
       "std        0.199489      0.226864            0.247019      0.055069   \n",
       "min        0.000000      0.000000            0.000000      0.000000   \n",
       "25%        0.252988      0.148778            0.333333      0.036802   \n",
       "50%        0.581673      0.182784            0.549020      0.054046   \n",
       "75%        0.631474      0.550478            0.705882      0.080269   \n",
       "max        1.000000      1.000000            1.000000      1.000000   \n",
       "\n",
       "       total_bedrooms    population    households  median_income  \n",
       "count    16512.000000  16512.000000  16512.000000   16512.000000  \n",
       "mean         0.086265      0.087201      0.093008       0.231966  \n",
       "std          0.066894      0.067101      0.070631       0.130148  \n",
       "min          0.000000      0.000000      0.000000       0.000000  \n",
       "25%          0.047519      0.047975      0.051904       0.142095  \n",
       "50%          0.069749      0.071227      0.076176       0.209301  \n",
       "75%          0.103898      0.105767      0.112957       0.292320  \n",
       "max          1.000000      1.000000      1.000000       1.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data_scaler = MinMaxScaler()\n",
    "scaled_data = data_scaler.fit_transform(imputed_num_data)\n",
    "\n",
    "# convert back to DataFrame so we can get a statistical description for comparison\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=num_data.columns)\n",
    "scaled_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assembling the Final Pipeline\n",
    "Using the the various different pieces of pipe (the transformations explored above), we assemble a final pipeline that can be used for preparing data before training ML algorithms or scoring (predicting) new feature instances (or observations).\n",
    "\n",
    "Note, that in this particular instance our final pipeline is actually a union of two seperate pipelines. This is because we have to treat numerical and catergorical variables seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "numeric_cols = X.select_dtypes(exclude=['object']).columns\n",
    "numeric_pipeline = Pipeline([\n",
    "    ('var_selector', DataFrameAdapter(numeric_cols)),\n",
    "    ('imputer', Imputer(strategy='median')),\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('var_selector', DataFrameAdapter(categorical_cols)),\n",
    "    ('categorical_encoder', CategoricalFeatureEncoder())\n",
    "])\n",
    "\n",
    "data_prep_pipeline = FeatureUnion([\n",
    "    ('numeric_pipeline', numeric_pipeline),\n",
    "    ('categorical_pipeline', categorical_pipeline)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Final Pipeline\n",
    "To make sure that our data preparation pipeline works as we expect it to, we apply it to the initial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepared data has 16512 observations of 13 features\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.20517928,  0.56004251,  0.94117647, ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [ 0.08266932,  0.90010627,  0.31372549, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.14541833,  0.87353879,  0.25490196, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ..., \n",
       "       [ 0.25498008,  0.47715197,  0.33333333, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.21115538,  0.77789586,  0.39215686, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.72410359,  0.02019129,  0.68627451, ...,  0.        ,\n",
       "         0.        ,  1.        ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_data = data_prep_pipeline.fit_transform(X)\n",
    "\n",
    "print('prepared data has {} observations of {} features'.format(*prepared_data.shape))\n",
    "prepared_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persisting the Pipeline to Disk\n",
    "Our chosen approach is to serialise it using SciKit-Learn's (actually SciPy's) `joblib` object serialiser as this is meant to work with large Numpy arrays better than Python's core `pickle` module does. It is also easier to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/data_prep_pipeline.pkl']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "joblib.dump(data_prep_pipeline, 'models/data_prep_pipeline.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check we can reload the model and use it as above\n",
    "deser_data_prep_pl = joblib.load('models/data_prep_pipeline.pkl') \n",
    "\n",
    "# verify that it gives the same results\n",
    "(deser_data_prep_pl.transform(X) == data_prep_pipeline.transform(X)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "collapse_to_match_collapsible_headings": false,
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "189px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
